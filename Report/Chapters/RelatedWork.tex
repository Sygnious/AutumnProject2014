\chapter{Related Work}
\label{cha:related-work}

There already exists many implementations of accelerators for various hashing
algorithms and DMA modules.

Accelerators for various hashing algorithms are often used in chips that need
access to quick cryptography implementations for various purposes. DMA modules are
found in most system-on-chip systems because of their obvious advantages.

\section{Hashing Accelerators}
\label{sec:previous-hash}
Many implementations of hashing accelerators exist and are in use in various
systems today. Because of this, there have been some research on different
architectures and optimizations for the accelerators.

Since the SHA-256 algorithm consists of a loop that applies the compression
algorithm described in section \ref{sec:sha-compr} to the extended message
block, it is tempting to look at ways of pipelining the algorithm to achieve
a higher throughput. However, as noted in \cite{sha-opt}, this is difficult
due to the way the algorithm is constructed. Nevertheless, \cite{dadda-sha}
presents a quasi-pipelined design.

Without pipelining the design, there are other optimizations suggested in
\cite{sha-opt}, such as unrolling the core loop of the algorithm, which
increases the throughput of the module but creates a longer critical path,
lowering the maximum clock frequency of the circuit.

Also suggested are moving certain operations to other stages of the algorithm
execution, using block RAM for the constants and using specific types of
adders or counters.

\section{DMA}
DMA modules are commonly found in systems-on-chips. Some such systems include the
PIC24F family of microcontrollers from Microchip, representing a common
single-core, low-power microcontroller, the Adapteva Epiphany, a more
exotic multicore architecture, and the Cell processor, which relies on complex DMA transfers between its processor cores.

\subsection{Microchip PIC24F device family}
Microchip Inc. is a provider of microcontrollers and analogue semiconductors \cite{microchip1}.
Among their products is the PIC24F device family, which includes a DMA engine.
The module is located on the microcontroller data bus between the CPU and DMA-enabled peripherals, with direct access to the SRAM.
The DMA controller is composed of multiple independent DMA channels.
Each channel can be independently programmed to transfer data between different areas of the data RAM, move data between single or multiple addresses, use a wide range of hardware triggers to initiate transfers, and conduct programmed transactions once or many times. 
Up to 16 channels are supported.
The communication network in the PIC24F is the SFR-bus, which is a shared bus.
All peripherals that request data transfers are connected to this bus.
The DMA module is controlled by a number of registers.
For the module itself, there are registers for the engine control, for upper and lower address limit, and a data buffer.
For each channel implemented, there are registers for channel control, channel interrupt control, registers for source and destination, and a transaction counter. 
All channels share the same data buffer for transfers \cite{microchip54}.

\subsection{Adapteva Epiphany}
The Epiphany architecture by Adapteva defines a multicore, scalable, shared-memory, parallel computing fabric.
It consists of a 2D-array of compute nodes connected by a low-latency mesh network-on-chip (NOC) called the eMesh.
Figure \ref{fig:AdaptevaEpiphany} shows an implementation of the architecture.
As can be seen in the figure, every node consists of a RISC CPU, local memory, a DMA engine and a network interface.
Every node is connected to a router.
The Epiphany architecture was designed for good performance across a broad range of applications, but excels at applications with high spatial and temporal locality of data and code.
Examples are image processing, communication, sensor signal processing, encryption and compression.
The 2D eMesh NOC that is used in this system handles traffic patterns in high-throughput real-time applications efficiently.

The DMA engine is custom designed for the eMesh fabric, and operates at the same speed as the eMesh, so that it generates a double word transaction every clock cycle.
Every node in the Epiphany has a DMA engine.

The main features of the DMA engine are: 
\begin{itemize}
    \item Two independent channels per processor node
    \item Separate specification of source/destination address configuration per descriptor and channel
    \item 2D DMA operations
    \item Flexible stride sizes
    \item DMA descriptor chaining and hardware interrupts flagging to local CPU subsystem
\end{itemize}

The DMA transfer types are local to external memory, external to local memory, transfer to local data in slave mode, and transfer between two external sources.

DMA descriptors are stored in local memory, and is brought into the DMA channel configuration register when the DMA transfer is activated.
Among the configuration are the source and destination addresses, strides and counter.
Channel 0 has higher priority than channel 1 \cite{epiphany}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\textwidth]{Figures/DMA/AdaptevaEpiphany}
    \caption{An implementation of the Epiphany Architecture \cite{epiphany}}
    \label{fig:AdaptevaEpiphany}
\end{figure}


\subsection{Cell Processor}
Kistler, Perrone and Petrini takes a look at the Cell Multiprocessor, developed by IBM, Sony and Toshiba, in their paper ``Cell Communication Network: Built for Speed'' \cite{cell}.
As multicores have become a major trend in computer architecture, the authors stress the need for on-chip networks that provide high performance in latency and bandwidth, if the computational power of the many available processing units are to be fully realized.
In this paper, the authors describe the architecture of the Cell Multiprocessor, and their experiments where they test the performance of the communication network.
One of the main components involved in the communication is the DMA engine.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\textwidth]{Figures/DMA/CellTop}
    \caption{Overview of the Cell Architecture \cite{cell}}
    \label{fig:CellTop}
\end{figure}

The Cell's architecture can bee seen in figure \ref{fig:CellTop}.
The Cell processor consists of a Power Processor Element (PPE) and 8 specialized coprocessors called Synergetic Processor Elements (SPE).
The PPE is the Cell's main processor.
It is a traditional 64-bit PowerPC processor core with a vector multimedia extention (VMX) unit.
It runs the operating system, and coordinates the SPEs.
Each SPE consists of a synergetic processing unit (SPU) that runs SIMD instructions and a memory flow controller (MFC).
Each MFC consists of a DMA controller, a memory management unit (MMU), a bus interface and an atomic unit for interfacing with other SPUs and the PPE.
The MFC connects the SPE to the element interconnect bus (EIB), which is the interconnect network that connects all the SPEs and the PPE.
There are several types of communication used for different purposes, but most of the communication between the SPU and other Cell elements is done through the DMA engine found on the MFC.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\textwidth]{Figures/DMA/CellDMA}
    \caption{Basic flow of a DMA transfer on the Cell, as seen from \cite{cell}}
    \label{fig:CellDMA}
\end{figure}

The DMA controller and the flow of a DMA transfer are illustrated in \ref{fig:CellDMA}.
The commands issued for the DMA modules are gets and puts (equivalent to loads and stores).
A DMA command can also be a list command, which requests multiple DMA transfers.
These are the steps of the DMA flow, as seen in figure \ref{fig:CellDMA}:

\begin{enumerate}
    \item DMA command is placed in the MFC SPU command queue through the channel interface.
    \item DMA controller selects a command for processing. DMA SPU queue has priority over the proxy queue (commands from PPE or other devices).
    \item If the DMA command is a list command that requires a list element, the DMA controller must fetch the element from the local-store interface. 
    The DMA entry is updated and must be reselected.
    \item If required, due to use of virtual memory, the command is queued in the MMU, which performs the address translation (either through the TLB or the page table in main memory) and updates the DMA entry. 
    It must then be reselected by the DMA controller from the queue.
    \item The DMA controller unrolls the command.
    Unrolling means that it creates bus requests to transfer the next block of data for the command.
    Each bus request can transfer up to 128 bytes of data.
    The bus requests are queued in the to the Bus Interface Unit (BIU).
    \item The BIU selects a bus request, and issues it to the EIB, where the command is queued with others.
    If main memory is involved, the MIC must acknowledge.
    \item When the EIB has given permission, the BIU will perform the reads to local store. % "reads to local store" makes no sense
    Memory from the MIC is transfered by the EIB.
    \item The unrolling process produces a series of bus requests for the DMA command that are pipelined through the communication network.
    The DMA command remains in the SPU DMA queue until all the bus requests have been completed.
    The DMA controller may still process other DMA commands.
    The SPU is notified once all bus requests are completed, and the command is removed from the queue. 
\end{enumerate}

When Kistler, Perrone and Petrini tested the performance of the communication system, they found that when using DMA from only one SPE, the highest possible bandwith when using only one SPE was measured up to 25~GB/s if message sizes were large enough, and latency below 100~ns if packet sizes were small enough.
The results were dependent on type, as gets or sets towards main memory or local store, and how many commands they allowed to be executed at the same time before blocking.

